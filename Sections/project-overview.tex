\section{Project Overview}
\sloppy

\subsection{Introduction and Motivation}
Natural Language Processing (NLP) has entered a new era with the advent of pre-trained language models, paving the way for constructing robust language models. Pretrained transformer-based models such as GPT-2 have become prevalent due to their cutting-edge efficiency. However, these approaches rely heavily on resource-intensive languages, forcing other languages to adopt multilingual frameworks (mGPT).

Recent studies show that language-specific GPT models outperform multilingual mGPT models. In this research, we propose a pretrained monolingual GPT model called Boishakh using the objective of causal language modeling (CLM). Due to the lack of available large datasets for NLP tasks in Bengali, we have created a Bengali language model dataset called BengaliCLM using a Bengali corpus scraped from several public websites. We employ a subword-based tokenization algorithm named Byte-Pair Encoding (BPE) for Bengali and train the Bengali-GPT2 model from scratch using the BengaliCLM dataset.

\subsection{Background and Significance}
Bengali ranks as the seventh most spoken language worldwide based on the number of speakers. A vast amount of Bengali content is generated from online news portals, emails, and social networking sites, necessitating the development of tools and techniques to process Bengali content automatically. Furthermore, many Bengali literature enthusiasts experience a void due to the absence of a fine-tuned AI model to satisfy their needs for literary inquiry.

\subsection{Project Description}
Boishakh is designed to cater to the needs of literary minds, students, and educators who take deep interest in learning about the Bengali language and its treasured literature. The project consists of several components:

\begin{itemize}
    \item A primary AI conversational model trained for:
    \begin{itemize}
        \item Summarizing literary works
        \item Providing academic support
        \item Acting as a virtual historian
    \end{itemize}
    \item A personal library system for users
    \item AI-assisted search functionality for finding books by author or title
\end{itemize}

\subsection{Technical Architecture}
The main component of Project Boishakh is the conversational assistant, built on the GPT-2 model architecture. While sequence-to-sequence models employing recurrent neural network (RNN) or long-short-term memory (LSTM) units were previously used for text generation, transformer-based pretrained models have now become the state-of-the-art solution. However, training transformer-based models presents several challenges:

\begin{itemize}
    \item High computational resource requirements
    \item Need for extensive training data
    \item Limited availability of resources for low-resource languages like Bengali
\end{itemize}

\subsection{Implementation Approach}
The language-specific transformer-based GPT-2 model, a descendant of the original GPT model, is renowned for its ingenious efficacy in text generation. As no pretrained GPT-2 model for the Bengali language existed in the literature, our primary objective was to develop one named Boishakh.

The implementation process involved:
\begin{enumerate}
    \item Creation of a large Bengali corpus
    \item Data cleaning through:
    \begin{itemize}
        \item Filtering HTML tags and non-Bengali content
        \item Applying Unicode normalization
        \item Implementing rule-based replacements
    \end{itemize}
    \item Deployment of Byte-Pair Encoding (BPE) for tokenization
    \item Training the BengaliGPT model using the preprocessed BengaliCLM dataset
\end{enumerate}

This implementation enables the model to effectively predict subsequent words from given input text, forming the foundation for our conversational AI system.
\section{Project Overview}
\sloppy
Natural Language Processing (NLP) has entered a new era with the advent of pre-trained language models, paving the way for constructing robust language models. Pretrained transformer-based models such as GPT-2 have become prevalent due to their cutting-edge efficiency. However, these approaches rely heavily on resource-intensive languages, forcing other languages to adopt multilingual frameworks (mGPT).

Recent studies show that the language-specific GPT model outperforms the multilingual mGPT model. In this research, we have proposed a pretrained monolingual GPT model called Boishakh using the objective of causal language modeling (CLM). Due to the lack of available large datasets for NLP tasks in Bengali, we have created a Bengali language model dataset called BengaliCLM using a Bengali corpus scraped from several public websites. We have used a subword-based tokenization algorithm named Byte-Pair Encoding (BPE) for Bengali and finally trained the Bengali-GPT2 model from scratch using the BengaliCLM dataset.

Based on the number of speakers, Bengali ranks as the seventh most spoken language worldwide. Moreover, a vast amount of Bengali content is generated from online news portals, emails, and social networking sites. Therefore, it is required to develop the tools and techniques to process Bengali content automatically. . And there are many Bengali literature enthusiasts who definitely feel the void for the absence of a fine-tuned AI model to satisfy their needs for literary inquiry.

Boishakh is created to cater to with the mindset of catering to the needs of the literary minds, students and educators alike who take deep interest in learning about the Bengali language and its treasured literature. Project Boishakh consists of several products, of which the AI conversational model named Boishakh is the prime feature. The conversational assistant has been particularly trained for summarizing literary works, providing academic support, and can also act as a virtual historian. It also consists of a personal library for the users along with an AI assisted search for finding context of books from either the author name or by the title name.

The main component of Project Boishakh is the conversational assistant, which consists of a GPT-2 model architecture. In the past, sequence-to-sequence models employing recurrent neural network (RNN) or long-short-term memory (LSTM) units have been employed for text generation. Recently,the transformer-based pretrained models have become the state-of-the-art for text generation. However, training transformer-based models with billions of parameters is costly , considering time and computational resources.It also needs a lot of training data, which is unavailable for languages with limited resources. Therefore, a low-resource language like Bengali adopt multilingual frameworks like mGPT. Multilingual mdodels are trained on a diverse dataset spanning multiple existing languages worldwide and perform poorly on low resource languages such as Bengali.

The language-specific transformer-based GPT-2 model, a descendant of the original GPTmodel, is a text generation model renowned for its ingenious efficacy. Unfortunately, no pretrained GPT-2 model for the Bengali language has been found in the literature. The main objective of this research is to develop a pretrained GPT-2 model for Bengali named Boishakh.

To train the model, we have created a large Bengali corpus. The dataset was cleaned by filtering out HTML tags and non-Bengali content , by applying Unicode normalization along with rule-based replacements to normalize the data. A subword-based tokenization algorithm named Byte-Pair Encoding (BPE) was deployed. Finally, the BengaliGPT model was trained using the preprocessed BengaliCLM dataset.This helps the model to predict the next word from given input text.

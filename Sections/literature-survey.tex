\section{Literature Review}

The creation of transformer architecture for the Boishakh LLM is a vast topic, requiring a review of various related works. Some of the research papers referred to during the development of the LLM model are as follows:

\subsection{Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation}

The authors in this research paper aimed to improve the performance of Bengali-English machine translation by addressing the low resource challenge for this language pair. Some of the methods implemented by them to achieve this are the creation of a high-quality Bengali-English parallel corpus comprising 2.75 million sentence pairs. A custom sentence segmenter for Bengali language was created to specifically tackle the nuances of the Bengali language. Aligner ensemble technique was used which combines the results of multiple sentence aligners to increase the number of correctly aligned sentence pairs extracted from parallel documents. The study found that ensembling significantly improved recall but also introduced incorrect alignments. To address this, a batch filtering technique was implemented along with a LASER toolkit to reduce the incorrect alignments.\\
The authors also developed a “Rising News” dataset to test the efficiency of the transformer model performance.

\subsection{BanglaGPT: A Model Based on Generative Pretrained Transformers for Bangla Language}

This document summarizes a research study focusing on the development of BanglaGPT, a monolingual, generative, pre-trained transformer-based model for the Bangla language. Existing multilingual pre-trained models like mGPT, though capable of handling Bengali language, show suboptimal performance as compared to the language-specific pre-trained models.\\
The researchers aimed to create a high-performing Bangla language model by addressing the scarcity of large Bangla datasets. They curated a BanglaCLM, comprising 26.24 GB of Bangla text sourced from various online platforms, including OSCAR, Wikipedia, and popular news websites like Prothom Alo and Kaler Kantho. The dataset underwent rigorous preprocessing, including the removal of non-Bangla content and HTML tags, followed by Unicode normalization using a dedicated Bangla normalizer library.
The researchers chose the GPT-2 model architecture, known for its effectiveness in text generation tasks, due to its reliance on the decoder component of the original transformer model. This architecture utilizes masked self-attention, feed-forward neural networks, and normalization blocks to process sequential data and predict the next token in a sequence.\\
The BanglaGPT model was trained using the BanglaCLM dataset, employing the Byte-Pair Encoding (BPE) tokenization algorithm and a context size of 128 tokens. The resulting BanglaGPT model achieved a perplexity score of 2.86 on the test set, demonstrating superior performance in text generation compared to both the mGPT model and an LSTM-based sequence-to-sequence model. Future research directions include applying BanglaGPT to downstream NLP tasks like text summarization and question answering, as well as exploring the potential of training a larger GPT-3 model using an expanded Bangla dataset.

\subsection{Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for Hindi}

The researchers in this paper mention the creation of a new, open source, Hindi centric large language model(LLM). It is a 10 billion parameter model based on the Llama 3 model. The developers curated a massive Hindi corpus of 65 billion tokens from sources such as websites, Wikipedia, news articles, and books. They also developed a specialized Hindi text processing pipeline to filter and clean the data.Nanda’s architecture is based on the Llama-3 model, using a decoder-only transformer architecture with enhancements like RoPE positional encoding and grouped-query attention. A custom tokenizer was developed to ensure balanced processing of both Hindi and English. A key innovation was the expansion of the transformer blocks from 32 to 40 to enable the model to integrate new domain and language-specific knowledge. Nanda was instruction-tuned using a bilingual dataset of English and Hindi in

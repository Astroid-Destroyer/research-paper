% Referenced from article 
@article{LlmPruner:2023,
  author  = {Ma, Xinyin and Gongfan Fang and Xinchao Wang},
  title   = {LLM-Pruner: On the Structural Pruning of Large Language Models},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2023},
  pages   = {21702-21720}
} 
 
% Referenced from article 
@article{BanglaGPT:2023,
  author  = {Md. Shahidul Salim and Hasan Murad and Dola Das and Faisal Ahmed},
  title   = {BanglaGPT: A Generative Pretrained Transformer-Based Model for Bangla 
             Language},
  journal = {Proceedings of the 2023 International Conference on Information and 
             Communication Technology for Sustainable Development (ICICT4SD)},
  year    = {2023},
  doi     = {10.1109/ICICT4SD59951.2023.10303383}
} 
 
% Referenced from article 
@article{BengaliMT:2020,
  author  = {Tahmid Hasan and Abhik Bhattacharjee and Kazi Samin Mubasshir and 
             Md. Hasan},
  title   = {Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New 
             Datasets for Bengali-English Machine Translation},
  journal = {arXiv preprint arXiv:2009.09359},
  year    = {2020},
  doi     = {10.48550/arXiv.2009.09359}
} 
 
% Referenced from article 
@article{Llama3:2024,
  author  = {Monojit Choudhury and Shivam Chauhan and Rocktim Jyoti Das and 
             others},
  title   = {Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for 
             Hindi},
  journal = {Cerebras Systems},
  year    = {2024}
} 
 
% Referenced from article 
@article{XLNet:2019,
  author  = {Zhilin Yang and Zihang Dai and others},
  title   = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  journal = {arXiv preprint arXiv:1906.08237},
  year    = {2019}
} 
 
% Referenced from article 
@article{WARM:2024,
  author  = {Alexandre Ramé and Nino Vieillard and Léonard Hussenot and others},
  title   = {WARM: On the Benefits of Weight Averaged Reward Models},
  journal = {arXiv preprint arXiv:2401.12187},
  year    = {2024},
  note    = {Submitted on 22 Jan 2024}
} 
 
% Referenced from article 
@article{TinyLlama:2024,
  author  = {Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
  title   = {TinyLlama: An Open-Source Small Language Model},
  journal = {arXiv preprint arXiv:2401.02385},
  year    = {2024},
  note    = {Last revised 4 Jun 2024}
} 
 
% Referenced from website 
@misc{InfosysNLP:2024,
  author       = {Infosys Springboard},
  title        = {Natural Language Processing with Infosys Springboard},
  howpublished = {\url{https://www.infosys.com/springboard/natural-language-processi
                  ng.html}},
  year         = {2024}
} 
 
% Referenced from article 
@inproceedings{TextProcessingHub:2024,
  author    = {P. Mishra and S. Bankar and M. Madankar},
  title     = {Text Processing Hub: NLP Applications},
  booktitle = {Proceedings of the 2024 International Conference on Innovations and 
               Challenges in Emerging Technologies (ICICET)},
  year      = {2024},
  pages     = {1-5},
  doi       = {10.1109/ICICET59348.2024.10616266}
} 
 
% Referenced from article 
@inproceedings{NLPTranslation:2024,
  author    = {N. Zade and G. Mate and K. Kishor and N. Rane and M. Jete},
  title     = {NLP Based Automated Text Summarization and Translation: A 
               Comprehensive Analysis},
  booktitle = {Proceedings of the 2024 2nd International Conference on Sustainable 
               Computing and Smart Systems (ICSCSS)},
  year      = {2024},
  pages     = {528-531},
  doi       = {10.1109/ICSCSS60660.2024.10624907}
} 
 
% Referenced from website 
@misc{AWSNLP,
  author       = {Amazon Web Services (AWS)},
  title        = {Natural Language Processing (NLP) on AWS},
  howpublished = {\url{https://aws.amazon.com/machine-learning/natural-language-pro
                  cessing/}},
  note         = {Accessed on 5/12/2024}
} 
 
% Referenced from website 
@misc{WikiRNN,
  author       = {Wikipedia},
  title        = {Recurrent Neural Network},
  howpublished = {\url{https://en.wikipedia.org/wiki/Recurrent_neural_network}},
  note         = {Accessed on 5/12/2024}
} 
 
% Referenced from website 
@misc{WikiLSTM,
  author       = {Wikipedia},
  title        = {Long Short-Term Memory},
  howpublished = {\url{https://en.wikipedia.org/wiki/Long_short-term_memory}},
  note         = {Accessed on 5/12/2024}
} 
 
% Referenced from website 
@misc{AWSRNN,
  author       = {Amazon Web Services (AWS)},
  title        = {Introduction to Recurrent Neural Networks},
  howpublished = {\url{https://aws.amazon.com/machine-learning/}},
  note         = {Accessed on 5/12/2024}
} 
% Referenced from website 
@misc{ResearchGateNN,
  title        = {A Brief Review of Neural Networks},
  howpublished = {\url{https://www.researchgate.net/publication/XXXXX}},
  note         = {Accessed on 6/12/2024}
} 